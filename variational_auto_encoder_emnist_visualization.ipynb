{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from variational_auto_encoder import VariationalAutoEncoder\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from utils import load_model, load_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the yaml file from /home/seungwon/Projects/deep-learning-projects/cv-04-variational-auto-encoder-pytorch/models/emnist/2023.07.05.14.04.01/config.yml\n",
      "Loaded the model from /home/seungwon/Projects/deep-learning-projects/cv-04-variational-auto-encoder-pytorch/models/emnist/2023.07.05.14.04.01/emnist_best.pt successfully\n",
      "VariationalAutoEncoder(\n",
      "  (encoder): Encoder(\n",
      "    (convs): ModuleList(\n",
      "      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (mu): Linear(in_features=3136, out_features=2, bias=True)\n",
      "    (log_var): Linear(in_features=3136, out_features=2, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (dense): Linear(in_features=2, out_features=3136, bias=True)\n",
      "    (convTs): ModuleList(\n",
      "      (0): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "      (3): ConvTranspose2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Set the configuration\n",
    "config = load_yaml(\"/home/seungwon/Projects/deep-learning-projects/cv-04-variational-auto-encoder-pytorch/models/emnist/2023.07.05.14.04.01/config.yml\")\n",
    "\n",
    "# Training setting\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.manual_seed(config['data']['seed'])\n",
    "if device == 'cuda':\n",
    "  torch.cuda.manual_seed_all(config['data']['seed'])\n",
    "\n",
    "# Set the transform\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Resize(config['data']['img_size'])])\n",
    "\n",
    "# Set the test data\n",
    "test_data = datasets.EMNIST(config['data']['data_path'],\n",
    "                            download=config['data']['download'],\n",
    "                            split='mnist',\n",
    "                            train=False,\n",
    "                            transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_data,\n",
    "                                          batch_size=config['data']['batch_size'],\n",
    "                                          shuffle=config['data']['shuffle'],\n",
    "                                          num_workers=config['data']['num_workers'],\n",
    "                                          drop_last=config['data']['drop_last'])\n",
    "\n",
    "# Set the model\n",
    "model = VariationalAutoEncoder(input_dim=config['model']['input_dim'], latent_z_dim=config['model']['latent_z_dim'],\n",
    "                                enc_conv_filters=config['model']['enc_conv_filters'], enc_conv_kernel=config['model']['enc_conv_kernel'],\n",
    "                                enc_conv_strides=config['model']['enc_conv_strides'], enc_conv_pad=config['model']['enc_conv_pad'],\n",
    "                                dec_convt_filters=config['model']['dec_convt_filters'], dec_convt_kernel=config['model']['dec_convt_kernel'],\n",
    "                                dec_convt_strides=config['model']['dec_convt_strides'], dec_convt_pad=config['model']['dec_convt_pad']).to(device)\n",
    "model, _, _, _ = load_model('/home/seungwon/Projects/deep-learning-projects/cv-04-variational-auto-encoder-pytorch/models/emnist/2023.07.05.14.04.01/emnist_best.pt', model)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seungwon/anaconda3/envs/cv/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/seungwon/Projects/deep-learning-projects/cv-04-variational-auto-encoder-pytorch/variational_auto_encoder_emnist_visualization.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmainserver1/home/seungwon/Projects/deep-learning-projects/cv-04-variational-auto-encoder-pytorch/variational_auto_encoder_emnist_visualization.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     sub \u001b[39m=\u001b[39m fig\u001b[39m.\u001b[39madd_subplot(\u001b[39m2\u001b[39m, n_to_show, i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmainserver1/home/seungwon/Projects/deep-learning-projects/cv-04-variational-auto-encoder-pytorch/variational_auto_encoder_emnist_visualization.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     sub\u001b[39m.\u001b[39maxis(\u001b[39m'\u001b[39m\u001b[39moff\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bmainserver1/home/seungwon/Projects/deep-learning-projects/cv-04-variational-auto-encoder-pytorch/variational_auto_encoder_emnist_visualization.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     sub\u001b[39m.\u001b[39;49mimshow(img, cmap\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mgray_r\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmainserver1/home/seungwon/Projects/deep-learning-projects/cv-04-variational-auto-encoder-pytorch/variational_auto_encoder_emnist_visualization.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_to_show):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmainserver1/home/seungwon/Projects/deep-learning-projects/cv-04-variational-auto-encoder-pytorch/variational_auto_encoder_emnist_visualization.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     img \u001b[39m=\u001b[39m outputs[i]\u001b[39m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m~/anaconda3/envs/cv/lib/python3.8/site-packages/matplotlib/_api/deprecation.py:459\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m name_idx:\n\u001b[1;32m    454\u001b[0m     warn_deprecated(\n\u001b[1;32m    455\u001b[0m         since, message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPassing the \u001b[39m\u001b[39m%(name)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%(obj_type)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    456\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpositionally is deprecated since Matplotlib \u001b[39m\u001b[39m%(since)s\u001b[39;00m\u001b[39m; the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    457\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mparameter will become keyword-only \u001b[39m\u001b[39m%(removal)s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    458\u001b[0m         name\u001b[39m=\u001b[39mname, obj_type\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/cv/lib/python3.8/site-packages/matplotlib/__init__.py:1414\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1411\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m   1412\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(ax, \u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1413\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1414\u001b[0m         \u001b[39mreturn\u001b[39;00m func(ax, \u001b[39m*\u001b[39;49m\u001b[39mmap\u001b[39;49m(sanitize_sequence, args), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1416\u001b[0m     bound \u001b[39m=\u001b[39m new_sig\u001b[39m.\u001b[39mbind(ax, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1417\u001b[0m     auto_label \u001b[39m=\u001b[39m (bound\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mget(label_namer)\n\u001b[1;32m   1418\u001b[0m                   \u001b[39mor\u001b[39;00m bound\u001b[39m.\u001b[39mkwargs\u001b[39m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m~/anaconda3/envs/cv/lib/python3.8/site-packages/matplotlib/axes/_axes.py:5487\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5480\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_aspect(aspect)\n\u001b[1;32m   5481\u001b[0m im \u001b[39m=\u001b[39m mimage\u001b[39m.\u001b[39mAxesImage(\u001b[39mself\u001b[39m, cmap, norm, interpolation,\n\u001b[1;32m   5482\u001b[0m                       origin, extent, filternorm\u001b[39m=\u001b[39mfilternorm,\n\u001b[1;32m   5483\u001b[0m                       filterrad\u001b[39m=\u001b[39mfilterrad, resample\u001b[39m=\u001b[39mresample,\n\u001b[1;32m   5484\u001b[0m                       interpolation_stage\u001b[39m=\u001b[39minterpolation_stage,\n\u001b[1;32m   5485\u001b[0m                       \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 5487\u001b[0m im\u001b[39m.\u001b[39;49mset_data(X)\n\u001b[1;32m   5488\u001b[0m im\u001b[39m.\u001b[39mset_alpha(alpha)\n\u001b[1;32m   5489\u001b[0m \u001b[39mif\u001b[39;00m im\u001b[39m.\u001b[39mget_clip_path() \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   5490\u001b[0m     \u001b[39m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cv/lib/python3.8/site-packages/matplotlib/image.py:702\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(A, PIL\u001b[39m.\u001b[39mImage\u001b[39m.\u001b[39mImage):\n\u001b[1;32m    701\u001b[0m     A \u001b[39m=\u001b[39m pil_to_array(A)  \u001b[39m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[0;32m--> 702\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39;49msafe_masked_invalid(A, copy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    704\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39muint8 \u001b[39mand\u001b[39;00m\n\u001b[1;32m    705\u001b[0m         \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mcan_cast(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype, \u001b[39mfloat\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msame_kind\u001b[39m\u001b[39m\"\u001b[39m)):\n\u001b[1;32m    706\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mImage data of dtype \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m cannot be converted to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    707\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mfloat\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype))\n",
      "File \u001b[0;32m~/anaconda3/envs/cv/lib/python3.8/site-packages/matplotlib/cbook/__init__.py:701\u001b[0m, in \u001b[0;36msafe_masked_invalid\u001b[0;34m(x, copy)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msafe_masked_invalid\u001b[39m(x, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 701\u001b[0m     x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49marray(x, subok\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[1;32m    702\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m x\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39misnative:\n\u001b[1;32m    703\u001b[0m         \u001b[39m# If we have already made a copy, do the byteswap in place, else make a\u001b[39;00m\n\u001b[1;32m    704\u001b[0m         \u001b[39m# copy with the byte order swapped.\u001b[39;00m\n\u001b[1;32m    705\u001b[0m         x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mbyteswap(inplace\u001b[39m=\u001b[39mcopy)\u001b[39m.\u001b[39mnewbyteorder(\u001b[39m'\u001b[39m\u001b[39mN\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# Swap to native order.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cv/lib/python3.8/site-packages/torch/_tensor.py:757\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[39m.\u001b[39m__array__, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m    756\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 757\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnumpy()\n\u001b[1;32m    758\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    759\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGkAAABpCAYAAAA5gg06AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABDUlEQVR4nO3RwQ3AIBDAsNL9dz5G4AmR7AkiZc3MfDztvx3AmUkBJgWYFGBSgEkBJgWYFGBSgEkBJgWYFGBSgEkBJgWYFGBSgEkBJgWYFGBSgEkBJgWYFGBSgEkBJgWYFGBSgEkBJgWYFGBSgEkBJgWYFGBSgEkBJgWYFGBSgEkBJgWYFGBSgEkBJgWYFGBSgEkBJgWYFGBSgEkBJgWYFGBSgEkBJgWYFGBSgEkBJgWYFGBSgEkBJgWYFGBSgEkBJgWYFGBSgEkBJgWYFGBSgEkBJgWYFGBSgEkBJgWYFGBSgEkBJgWYFGBSgEkBJgWYFGBSgEkBJgWYFGBSgEkBJgWYFGBSgEkBJgWYFLABo3EEzhsmCnIAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_to_show = 10\n",
    "np.random.seed(777)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 3))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "model.eval()\n",
    "\n",
    "for image, label in test_loader:\n",
    "    # Transfer data to device\n",
    "    inputs = image.to(device)\n",
    "    # Model inference\n",
    "    outputs, mu, log_var = model(inputs)\n",
    "\n",
    "    for i in range(n_to_show):        \n",
    "        img = inputs[i].squeeze()\n",
    "        sub = fig.add_subplot(2, n_to_show, i+1)\n",
    "        sub.axis('off')\n",
    "        sub.imshow(img, cmap='gray_r')\n",
    "\n",
    "    for i in range(n_to_show):\n",
    "        img = outputs[i].squeeze()\n",
    "        sub = fig.add_subplot(2, n_to_show, i+n_to_show+1)\n",
    "        sub.axis('off')\n",
    "        sub.imshow(img, cmap='gray_r')\n",
    "        \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
